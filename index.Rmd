--- 
title: "Limitations of Interpretable Machine Learning Methods"
author: ''
date: "`r Sys.Date()`"
colorlinks: yes
bibliography:
- book.bib
- packages.bib
description: Situations in which PDP, ALE, LIME, LOCO and feature importance fail.
documentclass: krantz
graphics: yes
link-citations: yes
lof: yes
lot: yes
site: bookdown::bookdown_site
biblio-style: apalike
---

```{r setup, include=FALSE}
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, width = 55, digits = 4
)

```

# Preface {-}

This project explains the limitations of current approaches in interpretable machine learning, such as partial dependence plots (PDP, Accumulated Local Effects (ALE), permutation feature importance, leave-one-covariate out (LOCO) and local interpretable model-agnostic explanations (LIME).
All of those methods can be used to explain the behavior and predictions of trained machine learning models.
The interpretation methods might not work well in the following cases:

- if a model models interactions (e.g. when a random forest is used)
- if features strongly correlate with each other
- if the model does not correctly model causal relationships
- if parameters of the interpretation method are not set correctly


## Structure of the book {-}

TODO



\mainmatter
