<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Introduction to Accumulated Local Effects (ALE) | Limitations of Interpretable Machine Learning Methods</title>
  <meta name="description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Introduction to Accumulated Local Effects (ALE) | Limitations of Interpretable Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Introduction to Accumulated Local Effects (ALE) | Limitations of Interpretable Machine Learning Methods" />
  
  <meta name="twitter:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-10-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pdp-causal.html"/>
<link rel="next" href="ale-pdp.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Limitations of ML Interpretability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-modeling-the-two-approaches"><i class="fa fa-check"></i><b>1.1</b> Statistical Modeling: The Two Approaches</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#importance-of-interpretability"><i class="fa fa-check"></i><b>1.2</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretable-machine-learning"><i class="fa fa-check"></i><b>1.3</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.4</b> Outline of the booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>2</b> Introduction to Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="2.1" data-path="pdp.html"><a href="pdp.html#partial-dependence-plots-pdp"><i class="fa fa-check"></i><b>2.1</b> Partial Dependence Plots (PDP)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="pdp.html"><a href="pdp.html#advantages-and-limitations-of-partial-dependence-plots"><i class="fa fa-check"></i><b>2.1.1</b> Advantages and Limitations of Partial Dependence Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="pdp.html"><a href="pdp.html#individual-conditional-expectation-curves"><i class="fa fa-check"></i><b>2.2</b> Individual Conditional Expectation Curves</a><ul>
<li class="chapter" data-level="2.2.1" data-path="pdp.html"><a href="pdp.html#centered-ice-plot"><i class="fa fa-check"></i><b>2.2.1</b> Centered ICE Plot</a></li>
<li class="chapter" data-level="2.2.2" data-path="pdp.html"><a href="pdp.html#derivative-ice-plot"><i class="fa fa-check"></i><b>2.2.2</b> Derivative ICE Plot</a></li>
<li class="chapter" data-level="2.2.3" data-path="pdp.html"><a href="pdp.html#advantages-and-limitations-of-ice-plots"><i class="fa fa-check"></i><b>2.2.3</b> Advantages and Limitations of ICE Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pdp-correlated.html"><a href="pdp-correlated.html"><i class="fa fa-check"></i><b>3</b> PDP and Correlated Features</a><ul>
<li class="chapter" data-level="3.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ProblemDescription"><i class="fa fa-check"></i><b>3.1</b> Problem Description</a><ul>
<li class="chapter" data-level="3.1.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#what-is-the-issue-with-dependent-features"><i class="fa fa-check"></i><b>3.1.1</b> What is the issue with dependent features?</a></li>
<li class="chapter" data-level="3.1.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#what-is-the-issue-with-extrapolation"><i class="fa fa-check"></i><b>3.1.2</b> What is the issue with extrapolation?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#RealData"><i class="fa fa-check"></i><b>3.2</b> Dependent Features: Bike Sharing Dataset</a><ul>
<li class="chapter" data-level="3.2.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#dependency-between-numerical-features"><i class="fa fa-check"></i><b>3.2.1</b> Dependency between Numerical Features</a></li>
<li class="chapter" data-level="3.2.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#dependency-between-categorical-features"><i class="fa fa-check"></i><b>3.2.2</b> Dependency between Categorical Features</a></li>
<li class="chapter" data-level="3.2.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#NumCat"><i class="fa fa-check"></i><b>3.2.3</b> Dependency between Numerical and Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#SimulatedData"><i class="fa fa-check"></i><b>3.3</b> Dependent Features: Simulated Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-settings-numerical-features"><i class="fa fa-check"></i><b>3.3.1</b> Simulation Settings: Numerical Features</a></li>
<li class="chapter" data-level="3.3.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-1-linear-dependence"><i class="fa fa-check"></i><b>3.3.2</b> Simulation of Setting 1: Linear Dependence</a></li>
<li class="chapter" data-level="3.3.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-2-nonlinear-dependence"><i class="fa fa-check"></i><b>3.3.3</b> Simulation of Setting 2: Nonlinear Dependence</a></li>
<li class="chapter" data-level="3.3.4" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-of-setting-3-missing-informative-feature-x_3"><i class="fa fa-check"></i><b>3.3.4</b> Simulation of Setting 3: Missing informative feature <span class="math inline">\(x_3\)</span></a></li>
<li class="chapter" data-level="3.3.5" data-path="pdp-correlated.html"><a href="pdp-correlated.html#simulation-settings-categorical-features"><i class="fa fa-check"></i><b>3.3.5</b> Simulation Settings: Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblem"><i class="fa fa-check"></i><b>3.4</b> Extrapolation Problem: Simulation</a><ul>
<li class="chapter" data-level="3.4.1" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblemEstablished"><i class="fa fa-check"></i><b>3.4.1</b> Simulation based on established learners</a></li>
<li class="chapter" data-level="3.4.2" data-path="pdp-correlated.html"><a href="pdp-correlated.html#ExtrapolationProblemPrediction"><i class="fa fa-check"></i><b>3.4.2</b> Simulation based on own prediction function</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="pdp-correlated.html"><a href="pdp-correlated.html#summary"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pdp-causal.html"><a href="pdp-causal.html"><i class="fa fa-check"></i><b>4</b> PDP and Causal Interpretation</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction.html"><a href="introduction.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="pdp-causal.html"><a href="pdp-causal.html#motivation"><i class="fa fa-check"></i><b>4.2</b> Motivation</a></li>
<li class="chapter" data-level="4.3" data-path="pdp-causal.html"><a href="pdp-causal.html#causal-interpretability-interventions-and-directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.3</b> Causal Interpretability: Interventions and Directed Acyclical Graphs</a></li>
<li class="chapter" data-level="4.4" data-path="pdp-causal.html"><a href="pdp-causal.html#scenarios"><i class="fa fa-check"></i><b>4.4</b> Scenarios</a></li>
<li class="chapter" data-level="4.5" data-path="pdp-causal.html"><a href="pdp-causal.html#conclusion"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5</b> Introduction to Accumulated Local Effects (ALE)</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp-causal.html"><a href="pdp-causal.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="ale.html"><a href="ale.html#ale-intro-formula"><i class="fa fa-check"></i><b>5.2</b> The Theoretical Formula</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ale.html"><a href="ale.html#centering"><i class="fa fa-check"></i><b>5.2.1</b> Centering</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html#estimation-formula"><i class="fa fa-check"></i><b>5.3</b> Estimation Formula</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#implementation-formula"><i class="fa fa-check"></i><b>5.3.1</b> Implementation Formula</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ale.html"><a href="ale.html#ale-intro-interpret"><i class="fa fa-check"></i><b>5.4</b> Intuition and Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ale-pdp.html"><a href="ale-pdp.html"><i class="fa fa-check"></i><b>6</b> Comparison of ALE and PDP</a><ul>
<li class="chapter" data-level="6.1" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-one-feature"><i class="fa fa-check"></i><b>6.1</b> Comparison one feature</a><ul>
<li class="chapter" data-level="6.1.1" data-path="ale-pdp.html"><a href="ale-pdp.html#example-1-multiplicative-prediction-function"><i class="fa fa-check"></i><b>6.1.1</b> Example 1: Multiplicative prediction function</a></li>
<li class="chapter" data-level="6.1.2" data-path="ale-pdp.html"><a href="ale-pdp.html#example-2-additive-prediction-function"><i class="fa fa-check"></i><b>6.1.2</b> Example 2: Additive prediction function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-two-features"><i class="fa fa-check"></i><b>6.2</b> Comparison two features</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ale-pdp.html"><a href="ale-pdp.html#the-2d-ale"><i class="fa fa-check"></i><b>6.2.1</b> The 2D ALE</a></li>
<li class="chapter" data-level="6.2.2" data-path="ale-pdp.html"><a href="ale-pdp.html#d-ale-vs-2d-pdp"><i class="fa fa-check"></i><b>6.2.2</b> 2D ALE vs 2D PDP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ale-pdp.html"><a href="ale-pdp.html#runtime-comparison"><i class="fa fa-check"></i><b>6.3</b> Runtime comparison</a><ul>
<li class="chapter" data-level="6.3.1" data-path="ale-pdp.html"><a href="ale-pdp.html#one-numerical-feature-of-interest"><i class="fa fa-check"></i><b>6.3.1</b> One numerical feature of interest</a></li>
<li class="chapter" data-level="6.3.2" data-path="ale-pdp.html"><a href="ale-pdp.html#two-numerical-features-of-interest"><i class="fa fa-check"></i><b>6.3.2</b> Two numerical features of interest</a></li>
<li class="chapter" data-level="6.3.3" data-path="ale-pdp.html"><a href="ale-pdp.html#one-categorial-feature-of-interest"><i class="fa fa-check"></i><b>6.3.3</b> One categorial feature of interest</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ale-pdp.html"><a href="ale-pdp.html#comparison-for-unevenly-distributed-data---example-4-munich-rents"><i class="fa fa-check"></i><b>6.4</b> Comparison for unevenly distributed data - Example 4: Munich rents</a></li>
<li class="chapter" data-level="6.5" data-path="ale-pdp.html"><a href="ale-pdp.html#appendix"><i class="fa fa-check"></i><b>6.5</b> Appendix</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ale-pdp.html"><a href="ale-pdp.html#ale-2d-example-calculation"><i class="fa fa-check"></i><b>6.5.1</b> Calculation of theoretical 2D ALE example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ale-misc.html"><a href="ale-misc.html"><i class="fa fa-check"></i><b>7</b> ALE Intervals, Piece-Wise Constant Models and Categorical Features</a><ul>
<li class="chapter" data-level="7.1" data-path="ale-misc.html"><a href="ale-misc.html#how-to-choose-the-number-andor-length-of-the-intervals"><i class="fa fa-check"></i><b>7.1</b> How to choose the number and/or length of the intervals</a><ul>
<li class="chapter" data-level="7.1.1" data-path="ale-misc.html"><a href="ale-misc.html#state-of-the-art"><i class="fa fa-check"></i><b>7.1.1</b> State of the art</a></li>
<li class="chapter" data-level="7.1.2" data-path="ale-misc.html"><a href="ale-misc.html#ale-approximations"><i class="fa fa-check"></i><b>7.1.2</b> ALE Approximations</a></li>
<li class="chapter" data-level="7.1.3" data-path="ale-misc.html"><a href="ale-misc.html#example-1-additive-feature-effects"><i class="fa fa-check"></i><b>7.1.3</b> Example 1: additive feature effects</a></li>
<li class="chapter" data-level="7.1.4" data-path="ale-misc.html"><a href="ale-misc.html#example-2-multiplicative-feature-effects"><i class="fa fa-check"></i><b>7.1.4</b> Example 2: multiplicative feature effects</a></li>
<li class="chapter" data-level="7.1.5" data-path="ale-misc.html"><a href="ale-misc.html#example-3-unbalanced-datasets-and-shaky-prediction-functions"><i class="fa fa-check"></i><b>7.1.5</b> Example 3: Unbalanced datasets and shaky prediction functions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ale-misc.html"><a href="ale-misc.html#problems-with-piece-wise-constant-models"><i class="fa fa-check"></i><b>7.2</b> Problems with piece-wise constant models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ale-misc.html"><a href="ale-misc.html#example-4-simple-step-function"><i class="fa fa-check"></i><b>7.2.1</b> Example 4: Simple step function</a></li>
<li class="chapter" data-level="7.2.2" data-path="ale-misc.html"><a href="ale-misc.html#example-5-two-dimensional-step-functions-and-unluckily-distributed-data"><i class="fa fa-check"></i><b>7.2.2</b> Example 5: Two-dimensional step functions and unluckily distributed data</a></li>
<li class="chapter" data-level="7.2.3" data-path="ale-misc.html"><a href="ale-misc.html#outlook"><i class="fa fa-check"></i><b>7.2.3</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ale-misc.html"><a href="ale-misc.html#categorical-features"><i class="fa fa-check"></i><b>7.3</b> Categorical Features</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ale-misc.html"><a href="ale-misc.html#ordering-the-features"><i class="fa fa-check"></i><b>7.3.1</b> Ordering the features</a></li>
<li class="chapter" data-level="7.3.2" data-path="ale-misc.html"><a href="ale-misc.html#estimation-of-the-ale"><i class="fa fa-check"></i><b>7.3.2</b> Estimation of the ALE</a></li>
<li class="chapter" data-level="7.3.3" data-path="ale-misc.html"><a href="ale-misc.html#example-of-ale-with-categorical-feature"><i class="fa fa-check"></i><b>7.3.3</b> Example of ALE with categorical feature</a></li>
<li class="chapter" data-level="7.3.4" data-path="ale-misc.html"><a href="ale-misc.html#interpretation"><i class="fa fa-check"></i><b>7.3.4</b> Interpretation</a></li>
<li class="chapter" data-level="7.3.5" data-path="ale-misc.html"><a href="ale-misc.html#changes-of-the-ale-due-to-different-orders"><i class="fa fa-check"></i><b>7.3.5</b> Changes of the ALE due to different orders</a></li>
<li class="chapter" data-level="7.3.6" data-path="pdp-causal.html"><a href="pdp-causal.html#conclusion"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pfi.html"><a href="pfi.html"><i class="fa fa-check"></i><b>8</b> Introduction to Feature Importance</a><ul>
<li class="chapter" data-level="8.1" data-path="pfi.html"><a href="pfi.html#permutation-feature-importance-pfi"><i class="fa fa-check"></i><b>8.1</b> Permutation Feature Importance (PFI)</a></li>
<li class="chapter" data-level="8.2" data-path="pfi.html"><a href="pfi.html#leave-one-covariate-out-loco"><i class="fa fa-check"></i><b>8.2</b> Leave-One-Covariate-Out (LOCO)</a></li>
<li class="chapter" data-level="8.3" data-path="pfi.html"><a href="pfi.html#interpretability-of-feature-importance-and-its-limitations"><i class="fa fa-check"></i><b>8.3</b> Interpretability of Feature Importance and its Limitations</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pfi-correlated.html"><a href="pfi-correlated.html"><i class="fa fa-check"></i><b>9</b> PFI, LOCO and Correlated Features</a><ul>
<li class="chapter" data-level="9.1" data-path="pfi-correlated.html"><a href="pfi-correlated.html#effect-on-feature-importance-by-adding-correlated-features"><i class="fa fa-check"></i><b>9.1</b> Effect on Feature Importance by Adding Correlated Features</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pfi-correlated.html"><a href="pfi-correlated.html#simulation"><i class="fa fa-check"></i><b>9.1.1</b> Simulation</a></li>
<li class="chapter" data-level="9.1.2" data-path="pfi-correlated.html"><a href="pfi-correlated.html#real-data"><i class="fa fa-check"></i><b>9.1.2</b> Real Data</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pfi-correlated.html"><a href="pfi-correlated.html#alternative-measures-dealing-with-correlated-features"><i class="fa fa-check"></i><b>9.2</b> Alternative Measures Dealing with Correlated Features</a></li>
<li class="chapter" data-level="9.3" data-path="pdp-correlated.html"><a href="pdp-correlated.html#summary"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
<li class="chapter" data-level="9.4" data-path="pfi-correlated.html"><a href="pfi-correlated.html#note-to-the-reader"><i class="fa fa-check"></i><b>9.4</b> Note to the reader</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pfi-partial.html"><a href="pfi-partial.html"><i class="fa fa-check"></i><b>10</b> Partial and Individual Permutation Feature Importance</a><ul>
<li class="chapter" data-level="10.1" data-path="pfi-partial.html"><a href="pfi-partial.html#ch2"><i class="fa fa-check"></i><b>10.1</b> Preliminaries on Partial and Individual Conditional Importance</a></li>
<li class="chapter" data-level="10.2" data-path="pfi-partial.html"><a href="pfi-partial.html#ch3"><i class="fa fa-check"></i><b>10.2</b> Simulations: A cookbook for using with PI and ICI</a><ul>
<li class="chapter" data-level="10.2.1" data-path="pfi-partial.html"><a href="pfi-partial.html#ch31"><i class="fa fa-check"></i><b>10.2.1</b> Detect Interactions</a></li>
<li class="chapter" data-level="10.2.2" data-path="pfi-partial.html"><a href="pfi-partial.html#ch32"><i class="fa fa-check"></i><b>10.2.2</b> Explain Interactions</a></li>
<li class="chapter" data-level="10.2.3" data-path="pfi-partial.html"><a href="pfi-partial.html#ch323"><i class="fa fa-check"></i><b>10.2.3</b> Stress Methods in a Non-Linear Relationship Setting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="pfi-partial.html"><a href="pfi-partial.html#ch4"><i class="fa fa-check"></i><b>10.3</b> Real Data Application: Boston Housing</a></li>
<li class="chapter" data-level="10.4" data-path="pfi-partial.html"><a href="pfi-partial.html#ch5"><i class="fa fa-check"></i><b>10.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pfi-data.html"><a href="pfi-data.html"><i class="fa fa-check"></i><b>11</b> PFI: Training vs. Test Data</a><ul>
<li class="chapter" data-level="11.1" data-path="pfi-data.html"><a href="pfi-data.html#introduction-to-test-vs.-training-data"><i class="fa fa-check"></i><b>11.1</b> Introduction to Test vs. Training Data</a></li>
<li class="chapter" data-level="11.2" data-path="pfi-data.html"><a href="pfi-data.html#theoretical-discussion-for-test-and-training-data"><i class="fa fa-check"></i><b>11.2</b> Theoretical Discussion for Test and Training Data</a></li>
<li class="chapter" data-level="11.3" data-path="pfi-data.html"><a href="pfi-data.html#reaction-to-model-behavior"><i class="fa fa-check"></i><b>11.3</b> Reaction to model behavior</a><ul>
<li class="chapter" data-level="11.3.1" data-path="pfi-data.html"><a href="pfi-data.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>11.3.1</b> Gradient Boosting Machines</a></li>
<li class="chapter" data-level="11.3.2" data-path="pfi-data.html"><a href="pfi-data.html#data-sets-used-for-calculations"><i class="fa fa-check"></i><b>11.3.2</b> Data sets used for calculations</a></li>
<li class="chapter" data-level="11.3.3" data-path="pfi-data.html"><a href="pfi-data.html#results"><i class="fa fa-check"></i><b>11.3.3</b> Results</a></li>
<li class="chapter" data-level="11.3.4" data-path="pfi-data.html"><a href="pfi-data.html#interpretation-of-the-results"><i class="fa fa-check"></i><b>11.3.4</b> Interpretation of the results</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="pdp-correlated.html"><a href="pdp-correlated.html#summary"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>12</b> Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</a><ul>
<li class="chapter" data-level="12.1" data-path="lime.html"><a href="lime.html#local-surrogate-models-and-lime"><i class="fa fa-check"></i><b>12.1</b> Local Surrogate Models and LIME</a></li>
<li class="chapter" data-level="12.2" data-path="lime.html"><a href="lime.html#how-lime-works-in-detail"><i class="fa fa-check"></i><b>12.2</b> How LIME works in detail</a><ul>
<li class="chapter" data-level="12.2.1" data-path="lime.html"><a href="lime.html#neighborhood"><i class="fa fa-check"></i><b>12.2.1</b> Neighborhood</a></li>
<li class="chapter" data-level="12.2.2" data-path="lime.html"><a href="lime.html#what-makes-a-good-explainer"><i class="fa fa-check"></i><b>12.2.2</b> What makes a good explainer?</a></li>
<li class="chapter" data-level="12.2.3" data-path="lime.html"><a href="lime.html#sampling-and-perturbation"><i class="fa fa-check"></i><b>12.2.3</b> Sampling and perturbation</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="lime.html"><a href="lime.html#example"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="ale-misc.html"><a href="ale-misc.html#outlook"><i class="fa fa-check"></i><b>12.4</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lime-neighbor.html"><a href="lime-neighbor.html"><i class="fa fa-check"></i><b>13</b> LIME and Neighborhood</a><ul>
<li class="chapter" data-level="13.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id2"><i class="fa fa-check"></i><b>13.1</b> The Neighborhood in LIME in more detail</a></li>
<li class="chapter" data-level="13.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id3"><i class="fa fa-check"></i><b>13.2</b> The problem in a one-dimensional setting</a></li>
<li class="chapter" data-level="13.3" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id4"><i class="fa fa-check"></i><b>13.3</b> The problem in more complex settings</a><ul>
<li class="chapter" data-level="13.3.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id41"><i class="fa fa-check"></i><b>13.3.1</b> Simulated data</a></li>
<li class="chapter" data-level="13.3.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id42"><i class="fa fa-check"></i><b>13.3.2</b> Real data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id5"><i class="fa fa-check"></i><b>13.4</b> Discussion and outlook</a></li>
<li class="chapter" data-level="13.5" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id6"><i class="fa fa-check"></i><b>13.5</b> Note to the reader</a><ul>
<li class="chapter" data-level="13.5.1" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id61"><i class="fa fa-check"></i><b>13.5.1</b> Packages used</a></li>
<li class="chapter" data-level="13.5.2" data-path="lime-neighbor.html"><a href="lime-neighbor.html#id62"><i class="fa fa-check"></i><b>13.5.2</b> How we used the lime R package and why</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lime-sample.html"><a href="lime-sample.html"><i class="fa fa-check"></i><b>14</b> LIME and Sampling</a><ul>
<li class="chapter" data-level="14.1" data-path="lime-sample.html"><a href="lime-sample.html#understanding-sampling-in-lime"><i class="fa fa-check"></i><b>14.1</b> Understanding sampling in LIME</a><ul>
<li class="chapter" data-level="14.1.1" data-path="lime-sample.html"><a href="lime-sample.html#formula"><i class="fa fa-check"></i><b>14.1.1</b> Formula</a></li>
<li class="chapter" data-level="14.1.2" data-path="lime-sample.html"><a href="lime-sample.html#sampling-strategies"><i class="fa fa-check"></i><b>14.1.2</b> Sampling strategies</a></li>
<li class="chapter" data-level="14.1.3" data-path="lime-sample.html"><a href="lime-sample.html#visualization-of-a-basic-example"><i class="fa fa-check"></i><b>14.1.3</b> Visualization of a basic example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="lime-sample.html"><a href="lime-sample.html#sketching-problems-of-sampling"><i class="fa fa-check"></i><b>14.2</b> Sketching Problems of Sampling</a></li>
<li class="chapter" data-level="14.3" data-path="lime-sample.html"><a href="lime-sample.html#real-world-problems-with-lime"><i class="fa fa-check"></i><b>14.3</b> Real World Problems with LIME</a><ul>
<li class="chapter" data-level="14.3.1" data-path="lime-sample.html"><a href="lime-sample.html#boston-housing-data"><i class="fa fa-check"></i><b>14.3.1</b> Boston Housing Data</a></li>
<li class="chapter" data-level="14.3.2" data-path="lime-sample.html"><a href="lime-sample.html#rental-bikes-data"><i class="fa fa-check"></i><b>14.3.2</b> Rental Bikes Data</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lime-sample.html"><a href="lime-sample.html#experiments-regarding-sampling-stability"><i class="fa fa-check"></i><b>14.4</b> Experiments regarding Sampling stability</a><ul>
<li class="chapter" data-level="14.4.1" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-feature-dimension"><i class="fa fa-check"></i><b>14.4.1</b> Influence of feature dimension</a></li>
<li class="chapter" data-level="14.4.2" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-sample-size"><i class="fa fa-check"></i><b>14.4.2</b> Influence of sample size</a></li>
<li class="chapter" data-level="14.4.3" data-path="lime-sample.html"><a href="lime-sample.html#influence-of-black-box"><i class="fa fa-check"></i><b>14.4.3</b> Influence of black box</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="ale-misc.html"><a href="ale-misc.html#outlook"><i class="fa fa-check"></i><b>14.5</b> Outlook</a></li>
<li class="chapter" data-level="14.6" data-path="pdp-causal.html"><a href="pdp-causal.html#conclusion"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Limitations of Interpretable Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ale" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Introduction to Accumulated Local Effects (ALE)</h1>
<p><em>Authors: Jakob Bodensteiner, Nikolas Fritz</em></p>
<p><em>Supervisor: Christian Scholbeck</em></p>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">5.1</span> Motivation</h2>
<p>As seen in section 2 PDPs don't work well as soon as two or more features are correlated. This gives rise to the definition of ALEs. Although their definition makes sense for high dimensional feature spaces including categorical features, within this section we only treat a space with two continuous features.</p>
</div>
<div id="ale-intro-formula" class="section level2">
<h2><span class="header-section-number">5.2</span> The Theoretical Formula</h2>
<p>The uncentered ALE with respect to a starting point <span class="math inline">\(z_{0, j}\)</span> is defined by <span class="citation">(Apley <a href="#ref-Apley2016">2016</a>)</span> as</p>
<p><span class="math display">\[  \widetilde{ALE}_{\hat{f},~j}(x) = \hat{f}_{x_j,ALE}(x) = \int_{z_{0,~j}}^{x} E_{X_c \mid X_j} [\hat{f}^j(X_j,~X_c)\mid X_j = z_j]~dz_j,\]</span> where <span class="math inline">\(\hat{f}\)</span> is an arbitrary prediction function, as well as <span class="math inline">\(\hat{f}^j(*,*)\)</span> its j-th partial derivative. In this context <span class="math inline">\(X_j\)</span> is the feature of interest while <span class="math inline">\(X_c\)</span> represents the other features.</p>
<div id="centering" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Centering</h3>
<p>The ALE (centered ALE) is defined as</p>
<p><span class="math display">\[  ALE_{\hat{f},~j}(x) = \widetilde{ALE}_{\hat{f},~j}(x) - E_{X_j}[\widetilde{ALE}_{\hat{f},~j}(X_j)]\]</span></p>
<p>The centering makes sense as it helps to interpret the ALE in a reasonable way. This will be explained in section <a href="ale.html#ale-intro-interpret">5.4</a>.</p>
</div>
</div>
<div id="estimation-formula" class="section level2">
<h2><span class="header-section-number">5.3</span> Estimation Formula</h2>
<p>Since this theoretical formula is of no use for a black box model with unknown or even non-existing gradients, an approximative approach will be used. The uncentered ALE can be approximated by the formula</p>
<p><span class="math display">\[ \widehat{\widetilde{ALE}}_{\hat{f},~j}(x) = \int_{z_{0,~j}}^{x} \sum_{k=1}^{K}    \frac{1_{I_k}(x_j)}{n_j(k)}\sum_{i:x_j^{(i)}\in N_j(k)} \frac{[\hat{f}(z_{k,~j}, x_{\setminus j}^{(i)})-\hat{f}(z_{k-1,~j}, x_{\setminus j}^{(i)})]}{z_{k,~j}-z_{k-1,~j}}~dx_j~.  \]</span></p>
<p>In a first step the relevant dimension of the feature space is divided into K intervals beginning with the starting point <span class="math inline">\(z_{0, j}\)</span>. As it is not clear how to exactly divide the feature space, section <a href="ale-misc.html#ale-misc">7</a> deals with that question. The upper boundary of the k-th interval is denoted by <span class="math inline">\(z_{k, ~j}\)</span> as well as the lower boundary by <span class="math inline">\(z_{k-1, ~j}\)</span>. The half-open interval <span class="math inline">\(]z_{k-1,~j},~z_{k,~j}]\)</span> is defined as <span class="math inline">\(I_k\)</span>. <span class="math inline">\(N_j(k)\)</span> denotes the k-th interval, i.e. <span class="math inline">\(]z_{k-1,~j}, z_{k,~j}]\)</span> and <span class="math inline">\(n_j(k)\)</span> the total number of observations having the j-value within this interval. <span class="math inline">\(x_j^{(i)}\)</span> is the j-value of the i-th observation and correspondingly <span class="math inline">\(x_{\setminus j}^{(i)}\)</span> the values of the other features. The term on the right approximates the expected partial derivative within each interval. Therefore each instance within an interval is shifted to the upper and lower limit of the interval and the total difference of the prediction is calculated. Divided by the length of the interval this is a reasonable approximation for the &quot;local&quot; effect on the prediction, if the feature of interest changes (cet. par.). By averaging these approximations over all observations within the k-th interval, we receive a rough estimator for the term <span class="math inline">\(E_{X_c \mid X_j} [\hat{f}^j(X_j,~X_c)\mid X_j \in N_j(k)]\)</span>, which we take as constant effect for the k-th interval. By integrating over this step function, which represents the locally estimated derivatives, the (local) changes are accumulated. That's why the name Accumulated Local Effects is quite reasonable. The approximative formula for the centered ALE follows directly as</p>
<p><span class="math display">\[ \widehat{ALE}_{\hat{f},~j}(x) = \widehat{\widetilde{ALE}}_{\hat{f},~j}(x) - \frac{1}{n} \sum_{i=1}^{n} \widehat{\widetilde{ALE}}_{\hat{f},~j}(x_j^{(i)})~. 
 \]</span></p>
<div id="implementation-formula" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Implementation Formula</h3>
<p>As both the centered and the uncentered ALE estimations are piecewise linear functions (integration over a step function), one can first calculate the ALE at the interval boundaries and interpolate in a second step. Therefore the following formula proposed by <span class="citation">(Apley <a href="#ref-Apley2016">2016</a>, 11)</span> with slightly changed notation will be useful. The definitions of its components are as above. Additionally <span class="math inline">\(k_j(x)\)</span> is defined as the number of the interval that contains <span class="math inline">\(x\)</span>, i.e. <span class="math inline">\(x \in ~]z_{k_j(x)-1,~j},~z_{k_j(x),~j}]\)</span>.</p>
<p><span class="math display">\[  \widehat{\widetilde{ALE}}_{steps,~\hat{f},~j}(x) =  \sum_{k=1}^{k_j(x)}   \frac{1}{n_j(k)}\sum_{i:~x_j^{(i)}\in N_j(k)} [\hat{f}(z_{k,~j}, x_{\setminus j}^{(i)})-\hat{f}(z_{k-1,~j},~x_{\setminus j}^{(i)})].  \]</span> This formula returns a step function. The values in each interval are the accumulated values of the averaged total differences in each interval. To transfer this formula into the correct estimator of the uncentered ALE one has to linearely interpolate the points <span class="math inline">\((z_{k-1,~j},~\widehat{\widetilde{ALE}}_{steps,~\hat{f},~j}(z_{k-1,~j}))\)</span> with <span class="math inline">\(( z_{k,~j},\widehat{\widetilde{ALE}}_{steps,~ \hat{f},~j}(z_{k,~j}))\)</span> for <span class="math inline">\(k \in \{1, ..., K \}\)</span> and <span class="math inline">\(\widehat{\widetilde{ALE}}_{steps, \hat{f},j}(z_{0,~j}) = 0\)</span>.</p>
<p>Since in this formula there is no integral, it is easier to implement.</p>
</div>
</div>
<div id="ale-intro-interpret" class="section level2">
<h2><span class="header-section-number">5.4</span> Intuition and Interpretation</h2>
<p>As the former sections introduced the theoretical basics for the ALE, this section shall provide an intuition as well for the calculation method as for the interpretation. As described above, the local behavior of the model with respect to the variable of interest is estimated by moving the existing data points to the boundaries of their interval and evaluating the total difference of the prediction for the &quot;new&quot; data points. Figure <a href="ale.html#fig:dataALE">5.1</a> first offered by <span class="citation">(Molnar <a href="#ref-molnar2019">2019</a>)</span> gives a good intuition for this procedure.</p>
<div class="figure"><span id="fig:dataALE"></span>
<img src="images/ale_estimation_intuition.PNG" alt="The data points within the 4-th interval are shifted to the interval boundaries \(z_{3,~1}\) and \(z_{4,~1}\)." width="100%" />
<p class="caption">
FIGURE 5.1: The data points within the 4-th interval are shifted to the interval boundaries <span class="math inline">\(z_{3,~1}\)</span> and <span class="math inline">\(z_{4,~1}\)</span>.
</p>
</div>

<p>First one splits the total range of the variable of interest (in this case <span class="math inline">\(x_1\)</span>) to intervals of suitable size.<br />
For each interval the contained data points are moved to the interval boundaries. One gets twice as much &quot;simulated&quot; new data points as originally contained in each interval. The prediction function is now evaluated at these simulated points and the total difference of the prediction (for the given interval) is estimated as the mean change. Divided by the length of the interval one gets an estimation for the partial derivative within this interval. Theoretically one receives the uncentered ALE by integration over this step function. Technically in a first step the total change per interval is accumulated. In a second step linear interpolation at the interval boundaries simulates a constant change within each interval. Both variants lead to the same result.</p>
<p>As the evaluation is ideally done on relatively small intervals, on the one hand the local behavior of the model is estimated. On the other hand the covariance structure of the features is taken into account, as only &quot;realistic&quot; data points are simulated. This is in accordance with sampling from the conditional distribution.</p>
<p>In a last step the uncentered ALE is centered, i.e. shifted by a constant such that the expectation of the centered ALE is zero.</p>
<p>Figure <a href="ale.html#fig:aleEx">5.2</a> shows an example ALE which could match the data situation of Figure <a href="ale.html#fig:dataALE">5.1</a>.</p>
<div class="figure"><span id="fig:aleEx"></span>
<img src="images/ale_example.png" alt="ALE on basis of 5 intervals" width="100%" />
<p class="caption">
FIGURE 5.2: ALE on basis of 5 intervals
</p>
</div>

<p>To understand the interpretation of the ALE it can be useful to first have a look at the intuition behind the uncentered ALE. If the value of the uncentered ALE at <span class="math inline">\(x_1 = 2\)</span> equals <span class="math inline">\(1\)</span>, this means that if one samples a data point from the joint distribution of both features but only knows that <span class="math inline">\(x_1 = 2\)</span>, one would expect the prediction to be 1 higher than the average prediction for realistic data points at <span class="math inline">\(x_1 = z_{0,1}\)</span> (i.e. data points sampled from the conditional distribution at <span class="math inline">\(x_1 = z_{0,1}\)</span>). This expectation strongly depends on the reference point <span class="math inline">\(z_{0,1}\)</span>, which per definition is smaller than the smallest <span class="math inline">\(x_1\)</span>-value of the data. By subtracting the expectation of the uncentered ALE - which is the mean difference of the prediction of a data point from the joint distribution to the prediction of a realistic data point(i.e. from the conditional distribution) at <span class="math inline">\(x_1 = z_{0,1}\)</span> - the interpretation becomes a lot easier. If the value of the (centered) ALE at <span class="math inline">\(x_1 = 2\)</span> equals for example <span class="math inline">\(2\)</span>, this means that, if one samples a data point from the joint distribution of both features and <span class="math inline">\(x_1\)</span> equals 2, one would expect the 1st order effect of feature <span class="math inline">\(x_1\)</span> to be 2 higher than the average 1st order effect of this feature.</p>
<p>So far only the case of 2-dimensional feature spaces with one feature of interest was taken into account. In the following chapters methods and interpretation for ALE with two numeric features (second order effects) or one categorical feature will be in the focus. Furthermore we will have a look on the size of the intervals the data is evaluated on, which can be crucial for the expressiveness of the ALE.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Apley2016">
<p>Apley, Daniel W. 2016. <em>Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models</em>. <a href="https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf" class="uri">https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf</a>.</p>
</div>
<div id="ref-molnar2019">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pdp-causal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ale-pdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/iml_methods_limitations/edit/master/02-00-ale.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
